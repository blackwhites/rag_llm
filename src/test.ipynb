{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"database/technical_engineering_vector_store.json\", \"r\") as f:\n",
    "    chunks_data = json.load(f)\n",
    "id = 0\n",
    "chunks_data[f'{id}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This python script contains functions that collect data in form of .txt,.doc and parse them into paragraphs or full text fr\n",
    "\n",
    "#Imports\n",
    "from annotated_types import doc\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "#obtaining doucment and returning list of paragraphs\n",
    "def obtain_paragraphs(doc):  #we can add extra logic here for obtaining paragraphs and metadata for each paragraph and convert to Doc\n",
    "    return doc.paragraphs\n",
    "\n",
    "#funciton for obtaining document\n",
    "def get_document(filenames: list):\n",
    "    file_paths = [] #for storing file paths\n",
    "    file_names = [] #for storing file names for reducing duplicacy\n",
    "    overall_paragraphs = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        file_name = filename\n",
    "        file_path = os.path.join('../data/doc/', file_name)\n",
    "\n",
    "        # checking duplicates in files and skipping it \n",
    "        if file_name in file_names:\n",
    "            continue\n",
    "\n",
    "        #obtaining paragraphs from document files\n",
    "        doc = Document(file_path)   #reading document file \n",
    "        \n",
    "        paragraphs = obtain_paragraphs(doc) #getting paragraphs\n",
    "        print(paragraphs)\n",
    "\n",
    "        overall_paragraphs.append(paragraphs) #adding all paragraphs\n",
    "        file_paths.append(file_path)\n",
    "        file_names.append(file_name)\n",
    "\n",
    "    return overall_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This Python Script contains logic to chunk the documents / Text Parsed information using Langchains and other methods\n",
    "'''\n",
    "\n",
    "#import langchain with overlaps / size\n",
    "import langchain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def do_chunking(paragraph):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    )   \n",
    "    texts = text_splitter.create_documents([paragraph])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do data collection \n",
    "filenames = ['new.docx'] #list containing all document names\n",
    "documents = get_document(filenames)\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do data chunking\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    for i in doc:\n",
    "    \n",
    "    chunks = do_chunking(doc)\n",
    "    chunked_documents.append(chunks)\n",
    "print(chunked_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Generate embeddings for a text\n",
    "text = [\"Your text to embed goes here\",\"dzdfasfdsa\"]\n",
    "embeddings = model.encode(text)\n",
    "\n",
    "# 'embeddings' now contains the embeddings for your text\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for a text\n",
    "embeddings = model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MistralForCausalLM\n",
    "\n",
    "model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)\n",
    "\n",
    "from langchain_community.llms import HuggingFaceTextGenInference\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=\"http://127.0.0.1:8080/\",\n",
    "    max_new_tokens=512,\n",
    "    top_k=50,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "model = Llama2Chat(llm=llm)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "print(\n",
    "    chain.run(\n",
    "        text=\"What can I see in Vienna? Propose a few locations. Names only, no details.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching faiss\n",
    "#imports\n",
    "from elasticsearch import Elasticsearch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def search_faiss(index_name,question_vector):\n",
    "\n",
    "    #reading faiss file\n",
    "    index = faiss.read_index(f'database/{index_name}')\n",
    "\n",
    "    #reading text file\n",
    "    #load the dictionary from the JSON file\n",
    "    with open(f\"database/{index_name.split('.')[0]}.json\", \"r\") as f:\n",
    "        chunks_data = json.load(f)\n",
    "        \n",
    "    query_vector = question_vector\n",
    "\n",
    "    # perform the search\n",
    "    k = 3  # number of top results to retrieve\n",
    "    distances, vector_ids = index.search(np.array([query_vector]), k)\n",
    "\n",
    "    # retrieve the corresponding text and metadata for the top results\n",
    "    # (assuming you have stored the text and metadata in a separate data structure)\n",
    "    top_results = []\n",
    "    for id in vector_ids[0]:\n",
    "        # retrieve the text  for the result using the vector ID\n",
    "        text = chunks_data[f'{id}']\n",
    "        # top_results.append((text, distances[0][id]))\n",
    "        top_results.append(text)\n",
    "    return top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhava\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This python script contains logic for getting the hugging face model for \n",
    "'''\n",
    "#imports\n",
    "from embedding import sentence_embeddings\n",
    "# from search_vector_creation import search_faiss\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "#Ask user query\n",
    "user_input = input(\"Please enter the query\")\n",
    "question = user_input #for ease if in case i double use both\n",
    "\n",
    "'''\n",
    "FUTURE SCOPE :  WE CAN DO QUERY EXPANSION (GENERATION OF QUERIES USING LLM MODEL IT SELF :) )\n",
    "'''\n",
    "\n",
    "#Get the search retrivals from database like top 3\n",
    "'''\n",
    "converting query to embedded vector as faiss search only on vectors \n",
    "(though vector represents multi modal data, it dont care it search/saves only vectors)\n",
    "\n",
    "'''\n",
    "index_name = 'technical_engineering_vector_store.faiss'\n",
    "question_vector = sentence_embeddings([question])[0]\n",
    "retreived_results = search_faiss(index_name,question_vector)\n",
    "print(retreived_results) #print\n",
    "\n",
    "\n",
    "#Add template\n",
    "prompt_template ='''\n",
    "Your an assitant of technical engineer. \n",
    "You will take the context from the user as context\n",
    "also you will have option to look on what question  you are trying to answer based on the context provided.\n",
    "\n",
    "Restrictions :  Your restricted to use your own knowledge or your restricted to hallucinate , Try to answer from the context for question\n",
    "\n",
    "answer should include similarity score between context and answer generated from chatgpt by having \n",
    "(answer similarity_score : score) at end of answer\n",
    "context:{context}\n",
    "Question : {question}\n",
    "'''\n",
    "\n",
    "#Config Model\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "\n",
    "#Convey prompt to model and input the obtained 3 retreivals\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "def load_model(model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "    return tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": 'Result : '.join(retreived_results), \"question\": RunnablePassthrough()} \n",
    "    |prompt\n",
    "    |load_model(model)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "#result the created Answer from model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
